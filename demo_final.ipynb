{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import TrainingCallback\n",
    "\n",
    "def create_learnable_data(num_rows,\n",
    "                          num_cols,\n",
    "                          num_classes,\n",
    "                          target_accuracy,\n",
    "                          seed=1234,\n",
    "                          split=True):\n",
    "    \"\"\"Create a synthetic dataset and return a pandas df.\"\"\"\n",
    "    seed = int(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    num_rows = int(num_rows)\n",
    "    num_cols = int(num_cols)\n",
    "    num_classes = int(num_classes)\n",
    "    target = float(target_accuracy)\n",
    "\n",
    "    if num_classes > 0:\n",
    "        x, y = make_classification(\n",
    "            n_samples=num_rows,\n",
    "            n_features=num_cols,\n",
    "            n_informative=num_cols // 2,\n",
    "            n_redundant=num_cols // 10,\n",
    "            n_repeated=0,\n",
    "            n_classes=num_classes,\n",
    "            n_clusters_per_class=2,\n",
    "            flip_y=1 - target,\n",
    "            random_state=seed,\n",
    "        )\n",
    "    else:\n",
    "        x, y = make_regression(\n",
    "            n_samples=num_rows,\n",
    "            n_features=num_cols,\n",
    "            n_informative=num_cols // 2,\n",
    "            n_targets=1,\n",
    "            noise=0.1,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "    data = pd.DataFrame(x, columns=[f\"feature_{i}\" for i in range(num_cols)])\n",
    "    data[\"labels\"] = pd.Series(y, dtype=\"bool\")\n",
    "\n",
    "    return train_test_split(\n",
    "        data, test_size=0.25, random_state=seed,\n",
    "        stratify=data[\"labels\"]) if split else data\n",
    "\n",
    "\n",
    "def get_parquet_files(path, size=10):\n",
    "    \"\"\"Get all parquet parts from a directory.\"\"\"\n",
    "    size *= 10\n",
    "    files = sorted(glob.glob(path))\n",
    "    while size > len(files):\n",
    "        files = files + files\n",
    "    files = files[0:size]\n",
    "    return files\n",
    "\n",
    "def load_parquet_dataset(files):\n",
    "    \"\"\"Load all parquet files into a pandas df.\"\"\"\n",
    "    df = pd.read_parquet(files[0])\n",
    "    for i in tqdm.tqdm(range(1, len(files), 50)):\n",
    "        df = pd.concat((df, pd.read_parquet(files[i:i+50])))\n",
    "        memory_usage = df.memory_usage(deep=True).sum()/1e9\n",
    "        tqdm.tqdm.write(f\"Dataset size: {memory_usage} GB\")\n",
    "        if memory_usage > 12:\n",
    "            raise MemoryError(f\"Dataset too big to fit into memory!\")\n",
    "    return df\n",
    "\n",
    "class TqdmCallback(TrainingCallback):\n",
    "    \"\"\"Simple callback to print a progress bar\"\"\"\n",
    "    def __init__(self, num_samples: int) -> None:\n",
    "        self.num_samples = num_samples\n",
    "        super().__init__()\n",
    "\n",
    "    def before_training(self, model):\n",
    "        if xgb.rabit.get_rank() == 0:\n",
    "            self.pbar = tqdm.tqdm(total=self.num_samples)\n",
    "        return model\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if xgb.rabit.get_rank() == 0:\n",
    "            self.pbar.update(1)\n",
    "\n",
    "    def after_training(self, model):\n",
    "        if xgb.rabit.get_rank() == 0:\n",
    "            self.pbar.close()\n",
    "        return model\n",
    "\n",
    "data_path = f\"/home/ubuntu/data/classification.parquet/**/*.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two pandas dataframes\n",
    "train_df, test_df = create_learnable_data(num_rows=1000000,\n",
    "                                            num_cols=40,\n",
    "                                            num_classes=2,\n",
    "                                            target_accuracy=0.8,\n",
    "                                            seed=1234)\n",
    "\n",
    "# XGBoost config.\n",
    "xgboost_params = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "\n",
    "from xgboost import DMatrix, train\n",
    "\n",
    "def train_xgboost(config, train_df, test_df, progress_bar=True):\n",
    "    target_column = \"labels\"\n",
    "\n",
    "    train_x = train_df.drop(target_column, axis=1)\n",
    "    train_y = train_df[target_column]\n",
    "    test_x = test_df.drop(target_column, axis=1)\n",
    "    test_y = test_df[target_column]\n",
    "\n",
    "    train_set = DMatrix(train_x, train_y)\n",
    "    test_set = DMatrix(test_x, test_y)\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Train the classifier\n",
    "    bst = train(params=config,\n",
    "                dtrain=train_set,\n",
    "                evals=[(test_set, \"eval\")],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=False,\n",
    "                num_boost_round=25,\n",
    "                callbacks=[TqdmCallback(25)] if progress_bar else [])\n",
    "    print(f\"Total time taken: {time.time()-start_time}\")\n",
    "\n",
    "    model_path = \"model.xgb\"\n",
    "    bst.save_model(model_path)\n",
    "    print(\"Final validation error: {:.4f}\".format(\n",
    "        evals_result[\"eval\"][\"error\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost(xgboost_params, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost_ray import RayDMatrix, train, RayParams\n",
    "\n",
    "def train_xgboost_ray(config, train_df, test_df, ray_params, progress_bar=True):\n",
    "    target_column = \"labels\"\n",
    "\n",
    "    train_set = RayDMatrix(train_df, target_column)\n",
    "    test_set = RayDMatrix(test_df, target_column)\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Train the classifier\n",
    "    bst = train(params=config,\n",
    "                dtrain=train_set,\n",
    "                evals=[(test_set, \"eval\")],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=False,\n",
    "                num_boost_round=25,\n",
    "                callbacks=[TqdmCallback(25)] if progress_bar else [],\n",
    "                ray_params=ray_params)\n",
    "    print(f\"Total time taken: {time.time()-start_time}\")\n",
    "\n",
    "    model_path = \"model.xgb\"\n",
    "    bst.save_model(model_path)\n",
    "    print(\"Final validation error: {:.4f}\".format(\n",
    "        evals_result[\"eval\"][\"error\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost_ray(xgboost_params, train_df, test_df, RayParams(num_actors=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(address='ray://13.52.180.135:10001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_xgboost_ray_cluster():\n",
    "    train_files, test_files = create_learnable_data(num_rows=1000000,\n",
    "                                                num_cols=40,\n",
    "                                                num_classes=2,\n",
    "                                                target_accuracy=0.8,\n",
    "                                                seed=1234)\n",
    "    # XGBoost config.\n",
    "    xgboost_params = {\n",
    "        \"tree_method\": \"approx\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    }\n",
    "    train_xgboost_ray(xgboost_params, train_files, test_files, RayParams(num_actors=8))\n",
    "    return xgb.Booster(model_file=\"model.xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = ray.get(train_xgboost_ray_cluster.remote())\n",
    "bst.save_model(\"model.xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_xgboost(inference_df, bst):\n",
    "    inference_df = DMatrix(inference_df)\n",
    "\n",
    "    results = bst.predict(inference_df)\n",
    "\n",
    "total_time = time.time()\n",
    "\n",
    "bst = xgb.Booster(model_file=\"model.xgb\")\n",
    "files = get_parquet_files(data_path, size=30)\n",
    "df = load_parquet_dataset(files).drop([\"labels\", \"partition\"], axis=1)\n",
    "infer_xgboost(df, bst)\n",
    "\n",
    "print(f\"Total time taken: {time.time()-total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost_ray import predict\n",
    "\n",
    "def infer_xgboost_ray(inference_df, bst):\n",
    "    bst.feature_names = sorted(bst.feature_names)\n",
    "    inference_df = RayDMatrix(inference_df, ignore=[\"labels\", \"partition\"])\n",
    "\n",
    "    predict(bst, inference_df, ray_params=RayParams(num_actors=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def infer_xgboost_ray_cluster(bst):\n",
    "    files = get_parquet_files(data_path, size=30)\n",
    "    infer_xgboost_ray(files, bst)\n",
    "\n",
    "total_time = time.time()\n",
    "ray.get(infer_xgboost_ray_cluster.remote(bst))\n",
    "print(f\"Total time taken: {time.time()-total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "@ray.remote\n",
    "def tune_xgboost():\n",
    "    train_df, test_df = create_learnable_data(num_rows=1000000,\n",
    "                                                num_cols=40,\n",
    "                                                num_classes=2,\n",
    "                                                target_accuracy=0.8,\n",
    "                                                seed=1234)\n",
    "\n",
    "    # Set XGBoost config.\n",
    "    config = {\n",
    "        \"tree_method\": \"approx\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"subsample\": tune.uniform(0.5, 1.0),\n",
    "        \"max_depth\": tune.randint(1, 9)\n",
    "    }\n",
    "\n",
    "    ray_params = RayParams(\n",
    "        max_actor_restarts=1,\n",
    "        gpus_per_actor=0,\n",
    "        cpus_per_actor=2,\n",
    "        num_actors=8)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(train_xgboost_ray, train_df=train_df, test_df=test_df, ray_params=ray_params, progress_bar=False),\n",
    "        # Use the `get_tune_resources` helper function to set the resources.\n",
    "        resources_per_trial=ray_params.get_tune_resources(),\n",
    "        config=config,\n",
    "        num_samples=16,\n",
    "        metric=\"eval-error\",\n",
    "        mode=\"min\",\n",
    "        verbosity=1)\n",
    "\n",
    "    accuracy = 1. - analysis.best_result[\"eval-error\"]\n",
    "    print(f\"Best model parameters: {analysis.best_config}\")\n",
    "    print(f\"Best model total accuracy: {accuracy:.4f}\")\n",
    "    return analysis.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(tune_xgboost.remote())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9eea0ff067b77427763455ef41ca90df17cba76272fad157f1d1cd121a27a08e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('xgboost_demo': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}